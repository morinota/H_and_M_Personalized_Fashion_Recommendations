# 2022/04/07

- レコメンドを予測するのは、transaction_train.csvの最終週の翌週1週間の各ユーザの購入。
- 予測対象の一週間はどんな季節？
  - =>transaction_train.csvの最終週は2020年9月16～2020年9月22日。
  - よって、予測するのは2020年9月23日～2020年9月29日の7日間。
  - 前年度の同シーズンの売れ筋とか見てもいいかも。
- グルーピング毎に、各レコメンド手法のオフライン精度評価は異なる！
  - 以下は、グルーピング例
  - "sales_channel_id"が「1(オフライン)か2(オンライン)」のどちらが多いか。＝オンラインで購入しがちなユーザか、オフラインで購入しがちなユーザか。
  - アイテムの平均購入金額:
    - 普段高額なアイテムを買ってるユーザに、安いアイテムをレコメンドしても買わないんじゃない？逆もしかりかも。
  - "年齢層"
- レコメンド対象ユーザ164万人のうち、1万人弱、sales_channelがNaNの人がいる。＝＞transaction_train期間(2年間)でアイテム未購入の人は"1万人/164万人"　＝＞全ユーザの1%以下。
- transactionデータを1週間だけ切り取ってみると...
  - ->対象期間の7日間で、購入されたアイテムは19000/105000種類。
  - -> たった一週間だけど、全アイテムのだいたい1/5も買われてるんだ！
- product_codeカラムは、article_id(9桁)を粗くしたモノ(上6桁)。(＝＞同種のアイテムの違う色とかっぽい！)

# 2022/04/07

- ユーザの購買特性や、アイテムの販売トレンドを、時系列のグラフで描画してみる。
  - 若年層のユーザはこの時期にトランザクションが活発になる？
  - このカテゴリのアイテムは、この時期には売れてこの時期には売れなくなる？
- グルーピングのアイデア
  - アクセサリを買う事が多い人か、服を買うことが多い人か、下着を買うことが多い人か、etc.

# 2022/04/09

- 各レコメンド手法の結果をまとめて管理するRecommendResultsクラスを定義する。
  - Validationしやすくなりそう？
  - こういうグルーピングのユーザにはこのレコメンド手法の精度がよさそう！みたいな調査ができそう...!
  - そうすれば、最後のアンサンブルの重み付けの根拠になりそう！(ex. グルーピングで重み付けを変える！)
- ただ、実際に今手元にあるレコメンド結果達は、one-week hold-out validationを考慮した結果ではない(検証用データが学習に使われてる！)
  - なので、実際に検証する際は、one-week hold-out validationを考慮してレコメンド結果を作り直す必要がある。
  - 要するに学習用データをいじって再計算するだけ。
  - ＝＞だから結局、任意の期間の学習データを渡して、各レコメンド手法を実行するコードを書く必要がある！

# 2022/04/10

- Partitioned_validationが思った通りの挙動をしてくれない...
  - One-week holdout関数は上手く機能してる。
  - Datasetオブジェクトも問題なし。
  - Last Purchased Itemsも問題なし。
  - partitioned_validation関数の挙動が異なるみたい...
    - ＝＞map@kとap@kの計算がおかしいっぽい？
    - もしくはその下の処理？
  - ＝＞何が間違ってたのかまだ分かってないけど、とりあえず見本のNotebookの関数をとってきて、何とかパイプラインが正しい評価値を出せるようになった！
- 以降は、色んなレコメンド手法やグルーピング毎のValidationを進めていく！

# 2022/04/14

    - Google Drive以下のval_results_csvに検証用のレコメンド結果を保存していく。
      - last purchased items
      - TimeDecayingPopularity
      - Time is our best friend
      - Rule based by customer age
      - Trending Product
      - Implicit ALS
      - Byfone approach
      - Chris approach
      - Byfone and Chris combination
      - LSTM_item_features
      - SVD reRanking

# 2022/04/16

- 検証用データをまとめるクラスを定義する?
  - 検証用データが格納されたフォルダを指定して、globとか使う？
  - ファイル名＝＞アプローチ名を取得？

# 2022/04/17

- Discussionをチェック
  - **２がオンラインで、1は店舗！**

# 2022/04/19

- 画像データを使ったレコメンド！
- まず画像特徴量を取得＝＞ResNet18を使ってEncode！
- 続いてコサイン類似度に基づいて、k-NNで、各アイテムの近傍アイテムと類似度を取得！
  - ＝＞全アイテムやろうとすると、10時間くらいかかりそう！
  - ＝＞このプロセスは、トランザクションデータのない新規アイテムだけで試す！
- 学習データをオフラインのトランザクションだけに絞れば、もう少し改善される？=>ちょっとそうでもなさそう！

# 2022/04/21

- 画像特徴量を用いたコンテンツベースのレコメンド結果を作成(notebook)
- ロギングモジュールを定義(script)

# 2022/04/22

- 画像特徴量を用いたコンテンツベースのレコメンドscriptを作成。
- 良いブログを発見
  - https://zenn.dev/panyoriokome/scraps/d521e032f3be15
- レコメンドのPersonalization Levelに関して
  - Non-personalized、Semi-personalized、Personalizedの3段階。
  - 今は、Non-personalizedとPersonalizedをベースにしてる。
  - コンテンツベースフィルタリングで、Semi-Personalizedを試して見る?

# 2022/04/25

- ランク学習のCandidateを、他のPopularityベースのレコメンドアイテムで作成する？？
- ^Cの解決法があれば、Candidateの合成とか色々できそう?
- もしくはユーザ特徴量、アイテム特徴量を増やす？？
- とりあえず訓練用のNegativeは、候補数20までならOK！
  - でも候補数15の方がスコア高かった？
- PositiveデータとNegativeデータ
  - Positive：実際にトランザクションが発生した。y=1のデータ(=レコード)
  - Negative：y=0のデータ。何らかの方法で生成する必要あり。
    - CandidateアイテムをそのままNegativeデータとして使う?

訓練用のNegativeの「候補」よりも、予測用の「候補」をどれだけ、いかに作るかが重要っぽい??気づいた事は以下。

- =>予測セクションでは、各ユニークユーザ毎に、各「候補」アイテムの購入確率を予測する。
  - そして、それの上位12個をレコメンドする。
  - ＝＞従って、予測用の「候補」数がそもそも12個だったら、どんな購入確率が予測されようと、各ユーザ毎の「候補」12個が全てレコメンドされてしまう...。
  - =>レコメンドのオフライン精度は、ランク学習モデルではなく、「予測用の候補」の作成方法にのみ依存する！
- ＝＞なので、以下の事が言える。

  - ランク学習モデルを、レコメンド精度に貢献させる為には、「予測用の候補」を少なくとも13個以上にする必要がある。
    - 数が多いほど、**ランク学習モデルが活きる**！
    - つまりランク学習モデルの機能は、**各ユーザ毎の「予測用の候補」アイテムを取捨選択する為**のもの！
  - 「予測用の候補」をいくつ、どのように作るかがかなり重要。

上記を踏まえて、「予測用の候補」に関する考察。
  - 数について：
    - 少なくとも24個は欲しい??
    - 24個の候補から、ランク学習モデルで12個を選ぶ作戦。
  - 候補生成方法について：
    - time decayingとかbyfoneとかlast purchasedのレコメンド結果を、候補として使う??=>それをランク学習モデルで、取捨選択！

よって今後のレコメンド精度アップの為に工夫できそうな事は以下。
1. 「予測用の候補」の数、および作成方法の工夫。
2. ランク学習モデル自体の精度アップ
   1. ハイパーパラメータの調整
   2. 入力する特徴量を増やす。
      1. ユーザ特徴量
      2. アイテム特徴量
      3. ユーザ×アイテムの特徴量(トランザクションデータから?)

やったこと
- rank LGBMのリーク部分を修正。
  - 訓練用トランザクションログにはリークは発生してなかったが、「予測用の候補」を生成する為のトランザクションログにリークが発生してしまっていた。
  - =>解決！
- 「予測用の候補」の数、どれくらいまでいけそう??
  - =>5%サンプルで訓練したモデルに対して、ユニークユーザ当り20個はできそう。
    - 5%サンプルだろうがフルサンプルだろうが、予測セクションでは全ユニークユーザを用いる。
    - ＝＞だからフルサンプルの場合も、20個は問題なくできそう！でも結構時間かかる?